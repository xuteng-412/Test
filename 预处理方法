语音类数据常用的预处理方法

---

1. 音频清洗与格式标准化
目的：确保音频质量一致，移除噪声，统一格式以便后续处理。  
方法：
- 去噪：使用噪声抑制算法（如谱减法或 Wiener 滤波）去除背景噪声。
- 格式转换：将音频统一为单一采样率（如 16kHz 或 44.1kHz）和编码格式（如 WAV、单声道）。
- 静音裁剪：移除音频开头和结尾的静音部分以减少无关数据。
- 音量归一化：调整音频振幅（如 RMS 归一化）以统一音量范围。

工具与实现：
- Librosa：`librosa.load()` 加载音频，`librosa.util.normalize()` 归一化音量。
- PyAudioAnalysis：提供去噪和静音检测功能（[GitHub](https://github.com/tyiannak/pyAudioAnalysis)).
- SoX：命令行工具，用于批量格式转换和去噪（[SoX官网](http://sox.sourceforge.net/)).
- 示例：`librosa.load('audio.wav', sr=16000, mono=True)` 统一采样率并转为单声道。

适用场景：所有语音任务的初始步骤，尤其对 IEMOCAP、RAVDESS 等数据集，需统一采样率以适配模型输入。

---

 2. 分帧与窗函数
目的：将连续音频分割为短时帧（通常 20-40ms），便于提取时序特征。  
方法：
- 分帧：将音频切分为固定长度的帧（帧长 25ms，帧移 10ms），以捕捉短时平稳信号。
- 加窗：对每帧应用窗函数（如 Hann 或 Hamming 窗）以减少频谱泄漏。
- 重叠：帧间重叠（通常 50%）以平滑信号过渡。

工具与实现：
- Librosa：`librosa.util.frame()` 实现分帧，`scipy.signal.get_window('hann', frame_length)` 提供窗函数。
- PyTorch：`torchaudio.transforms.Spectrogram()` 自动处理分帧和加窗。
- 示例：`librosa.stft(audio, n_fft=512, hop_length=256, window='hann')` 提取短时傅里叶变换（STFT）。

适用场景：为频域特征提取（如 MFCC、Mel 谱）做准备，常用于 EMO-DB、TESS 等数据集。

---

 3. 特征提取
目的：从音频帧中提取与情感相关的特征，供机器学习模型使用。  
常用特征：
- 梅尔频率倒谱系数 (MFCC)：
  - 捕捉语音的音色和频谱特性，广泛用于 SER。
  - 通常提取 13-40 个系数，包含动态特征（一阶差分 Δ 和二阶差分 ΔΔ）。
  - 实现：`librosa.feature.mfcc(y=audio, sr=16000, n_mfcc=13)`.
- Mel 频谱图：
  - 表示音频的频率能量分布，适合卷积网络输入。
  - 实现：`librosa.feature.melspectrogram(y=audio, sr=16000, n_mels=128)`.
- 音高 (Pitch)：
  - 提取基频（F0）以反映语音的情感变化（如愤怒的高音调）。
  - 实现：`parselmouth.Sound(audio).to_pitch()`（[Parselmouth](https://github.com/YannickJadoul/Parselmouth)）。
- 对数能量 (Log Energy)：
  - 捕捉语音的强度变化，与情感强度相关。
  - 实现：`librosa.feature.rms(y=audio)`.
- Chroma 特征：
  - 表示音调的分布，适合歌曲或音乐相关情感任务。
  - 实现：`librosa.feature.chroma_stft(y=audio, sr=16000)`.
- 零交叉率 (ZCR)：
  - 测量信号过零次数，反映语音的频率特性。
  - 实现：`librosa.feature.zero_crossing_rate(y=audio)`.

工具与实现：
- Librosa：主流特征提取库（[GitHub](https://github.com/librosa/librosa)).
- Torchaudio：PyTorch 的音频处理模块，支持高效特征提取（[PyTorch](https://pytorch.org/audio/stable/)).
- OpenSMILE：专业语音特征提取工具，适合复杂特征集（[GitHub](https://github.com/audeering/opensmile)).

适用场景：MFCC 和 Mel 频谱图是 IEMOCAP、RAVDESS、ESD 等数据集的标准特征；音高和能量特征常用于细粒度情感分析。

---

 4. 数据增强
目的：增加数据集多样性，提升模型鲁棒性，模拟真实场景。  
方法：
- 加噪：添加白噪声或环境噪声（如咖啡馆背景音）。
  - 实现：`torchaudio.transforms.AddNoise()`.
- 音高变换：随机调整音高以模拟不同说话者。
  - 实现：`librosa.effects.pitch_shift(y=audio, sr=16000, n_steps=2)`.
- 时间拉伸：改变语速而不改变音高。
  - 实现：`librosa.effects.time_stretch(y=audio, rate=1.2)`.
- 音量扰动：随机调整音量。
  - 实现：`torchaudio.transforms.Vol(gain=0.5)`.

工具与实现：
- Audiomentations：Python 数据增强库（[GitHub](https://github.com/iver56/audiomentations)).
- Torchaudio：支持多种增强变换。
- 示例：`from audiomentations import AddGaussianNoise; augment = AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015)`.

适用场景：对小型数据集（如 EMO-DB、SAVEE）进行增强，改善模型对噪声或语速变化的泛化能力。

---

5. 数据标注与分割
目的：为监督学习准备带标签的数据，或分割长音频以便处理。  
方法：
- 情感标签对齐：确保音频片段与数据集提供的情感标签（如愤怒、悲伤）对齐。
  - 实现：使用数据集自带的元数据（如 CSV 或 JSON 文件）加载标签。
- 时间戳分割：将长音频按对话或句子分割为短片段。
  - 实现：`pydub.AudioSegment.from_file('audio.wav').split_to_segments()`（[Pydub](https://github.com/jiaaro/pydub)).
- 平衡数据集：调整各类情感样本数量，防止类别不平衡。
  - 实现：手动下采样或过采样，或使用 `imbalanced-learn` 库。

工具与实现：
- Pydub：音频分割和处理。
- Pandas：处理标签和元数据（`pandas.read_csv('labels.csv')`）.
- 示例：`from pydub import AudioSegment; audio = AudioSegment.from_file('audio.wav'); segments = audio[1000:5000]`（按时间戳切分）.

适用场景：IEMOCAP、MELD 等对话数据集需按时间戳分割；CREMA-D 等需检查标签一致性。

---

 6. 特征归一化与标准化
目的：统一特征尺度，提升模型训练稳定性。  
方法：
- Z-score 标准化：将特征缩放到均值为 0、标准差为 1。
  - 实现：`sklearn.preprocessing.StandardScaler`.
- Min-Max 归一化：将特征缩放到 [0,1] 或 [-1,1]。
  - 实现：`sklearn.preprocessing.MinMaxScaler`.
- 帧级归一化：对每帧特征单独归一化，适合时序模型。
  - 实现：`librosa.util.normalize(features, axis=0)`.

工具与实现：
- Scikit-learn：提供标准化工具（[Scikit-learn](https://scikit-learn.org/stable/)).
- NumPy：手动实现归一化（`features = (features - np.mean(features)) / np.std(features)`）.

适用场景：所有数据集的特征（如 MFCC、Mel 谱）需标准化，尤其是输入传统机器学习模型（如 SVM、K-means）时。

---

 7. 时序处理与序列化
目的：为时序模型（如 RNN、LSTM）准备连续帧序列。  
方法：
- 固定长度序列：将特征序列截断或填充到固定长度。
  - 实现：`torch.nn.utils.rnn.pad_sequence()` 或 `numpy.pad`.
- 滑动窗口：生成固定大小的特征窗口以捕捉时序上下文。
  - 实现：`librosa.util.frame(features, frame_length=50, hop_length=25)`.
- 时间掩码：为变长序列生成掩码，避免填充值影响模型。
  - 实现：`torch.nn.utils.rnn.pack_padded_sequence()`.

工具与实现：
- PyTorch：支持时序数据处理（[PyTorch](https://pytorch.org/)).
- TensorFlow：提供类似序列处理功能（[TensorFlow](https://www.tensorflow.org/)).

适用场景：IEMOCAP、MSP-Podcast 等长音频序列需处理为固定长度输入，适合 LSTM 或 CNN-LSTM 模型。

---

 推荐工具与资源
- Librosa：音频加载、特征提取、预处理一体化（[文档](https://librosa.org/doc/)).
- Torchaudio：与 PyTorch 集成的音频处理库，适合深度学习（[文档](https://pytorch.org/audio/stable/)).
- OpenSMILE：提取复杂特征集（如 eGeMAPS），适合情感分析（[文档](https://audeering.github.io/opensmile-python/)).
- Audiomentations：高效的数据增强库。
- Praat/Parselmouth：音高和韵律特征提取（[Praat](https://www.fon.hum.uva.nl/praat/))。

